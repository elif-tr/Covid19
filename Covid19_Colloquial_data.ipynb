{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Working with Colloquial datasets\n",
    "\n",
    "This notebook works with already tagged colloquial files that are provided. We will read in the existing 2 data sets and analyze the number of symptoms on each paper and create a one merged list of symptoms from both data sets.\n",
    "Steps taken on this notebook are as follows:\n",
    "\n",
    "    1. Read in patient-site_lableled.xlsx and plm_dataset_labeled.xlsx files\n",
    "    2. Merge them together into one tagged file\n",
    "    3. Exrtact the list of symptoms from both files\n",
    "    4. Provide the count of each symptom occurance in the files\n",
    "    5. Join sentence words together to retrive the full sentences and only selec the ones that include any of the sympmtoms from the symptom list\n",
    "    6. Remove sentences that have 1 or less characters in them.\n",
    "    7. After extracting the sentences that include one or more symptoms, split each sentence back to words format and then tag them\n",
    "    8. Final output file is the tagged version of both files combined and includes on sentences that contain any of the given symptms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import operator\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Patient Site data\n",
    "paSi_df = pd.read_excel('/Users/elif/Downloads/OneDrive_1_10-29-2020/patient-site_lableled.xlsx', sheet_name='in', usecols=\"A,B,C\")\n",
    "#Patience like me site data\n",
    "plm_df = pd.read_excel('/Users/elif/Downloads/OneDrive_1_10-29-2020/plm_dataset_labeled.xlsx', sheet_name='plm_dataset', usecols=\"A,B,C\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Combine both colloquial dataframes into one\n",
    "combined_colloquial_df = pd.concat([paSi_df,plm_df],ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use `colloq_data_symp_count` module to get a dictionary of marked symptoms and their counts of how many times they appear in the  tagged dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import Colloq_data_symp_count as coll_count\n",
    "plm_sym_dict = coll_count.symAndCount(plm_df)\n",
    "paSi_sym_dict = coll_count.symAndCount(paSi_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Full dictionary of symptoms collected from both tagged data files that are from Jul, 2020 and until Nov, 2020."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "new_dict = dict(Counter(plm_sym_dict) + Counter(paSi_sym_dict))\n",
    "#Sort the dictionary of symptoms along with their value counts\n",
    "\n",
    "sorted_d = dict( sorted(new_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# sorted_d\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a dataframe of the symptoms and their counts as well as a full list of symptoms that are collected from both colloquial datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Create a data frame for combined symptoms and how many times they appear in both colloquial tagged data\n",
    "df_sym_freq = pd.DataFrame(list((dict(new_dict)).items()), columns = ['Symptom', 'Frequency'])\n",
    "\n",
    "#Output the combined symptoms from both colloquial datasets\n",
    "df_sym_freq.to_csv('sym_freq.csv',index=False)\n",
    "\n",
    "#List of symptoms from colloquial datasets\n",
    "colloq_data_symps = list((dict(new_dict)).keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Wordcloud of symptoms\n",
    "\n",
    "# wc = WordCloud(background_color = \"black\", width = 1000, height= 1000).generate_from_frequencies(new_dict)\n",
    "# fig = plt.figure(figsize = (15,15))\n",
    "# plt.imshow(wc, interpolation = \"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "             Frequency\nSymptom               \ncough               41\nfever               32\nsore throat         16\nfatigue             15\nheadache            15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Frequency</th>\n    </tr>\n    <tr>\n      <th>Symptom</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cough</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>fever</th>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>sore throat</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>fatigue</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>headache</th>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sym_freq.groupby('Symptom').sum().sort_values(['Frequency'],ascending=False).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of symptoms retrived from both data sets is: 400\n"
     ]
    }
   ],
   "source": [
    "# colloq_data_symps\n",
    "print('Total number of symptoms retrived from both data sets is:', len(colloq_data_symps))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a version of the COLL-DATA that only contains *sentences* with symptom terms\n",
    "\n",
    "Use `colloq_tagged_data_processing` module to process tagged colloquial data frames and processes them to find sentences with symptoms."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import colloq_tagged_data_processing as coll_process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sent_value, colloquial_df = coll_process.colloquial_data_processing(combined_colloquial_df, colloq_data_symps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in the combined colloquial file is: 1384\n",
      "Number of sentences that include any of the provided symptoms is: 469\n",
      "Number of sentences that do NOT include any of the provided symptoms is: 915\n"
     ]
    }
   ],
   "source": [
    "def colloqual_df_info(sent, df):\n",
    "    print('Total number of sentences in the combined colloquial file is:', len(sent))\n",
    "    print('Number of sentences that include any of the provided symptoms is:', len(df))\n",
    "    print('Number of sentences that do NOT include any of the provided symptoms is:', (len(sent) - len(df)))\n",
    "\n",
    "colloqual_df_info(sent_value, colloquial_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Put the colloquial data sentences into tagged format\n",
    "\n",
    "Tagging was done on the combined dataframes from both sites and the symptoms used for tagging this datasets were those that extracted from the colloquial files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 469 entries, 0 to 468\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  469 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "colloquial_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "import split_text\n",
    "plm_colloq_df = split_text.sentence_w_symptoms(colloquial_df, colloq_data_symps)\n",
    "plm_colloq_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import symp_search\n",
    "colloq_df = symp_search.symptom_search(plm_colloq_df, colloq_data_symps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'regex' has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-1ecca3a80f9a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0msymptom_tagging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m#df5 = symptom_tagging.tokenize_sentences(plm_colloq_df)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mplm_peS_df_out\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0msymptom_tagging\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove_duplicate_sentence_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msymptom_tagging\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mplm_colloq_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Covid19/symptom_tagging.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m '''\n\u001B[1;32m      5\u001B[0m \u001B[0mModule\u001B[0m \u001B[0mthat\u001B[0m \u001B[0msplits\u001B[0m \u001B[0msentences\u001B[0m \u001B[0minto\u001B[0m \u001B[0mone\u001B[0m \u001B[0mword\u001B[0m \u001B[0mper\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mthen\u001B[0m \u001B[0mtags\u001B[0m \u001B[0mthem\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0msym\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0msym\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/site-packages/nltk/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrammar\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprobability\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtree\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/site-packages/nltk/text.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mf_measure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBigramAssocMeasures\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollocations\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mBigramCollocationFinder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msent_tokenize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m ConcordanceLine = namedtuple(\n",
      "\u001B[0;32m/usr/local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcasual\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTweetTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasual_tokenize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmwe\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mMWETokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdestructive\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mNLTKWordTokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/site-packages/nltk/tokenize/casual.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[0;31m# This is the core tokenizing regex:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 166\u001B[0;31m \u001B[0mWORD_RE\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mregex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr\"\"\"(%s)\"\"\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m\"|\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mREGEXPS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mregex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVERBOSE\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0mregex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mI\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0mregex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUNICODE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[0;31m# WORD_RE performs poorly on these patterns:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'regex' has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "import symptom_tagging\n",
    "#df5 = symptom_tagging.tokenize_sentences(plm_colloq_df)\n",
    "plm_peS_df_out= symptom_tagging.remove_duplicate_sentence_ids(symptom_tagging.tokenize_sentences(colloq_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Sentence  \\\n0    I ' ve had 2 sets of prohylactic antibiotics t...   \n1    I ' ve had severe neuropathy during the illnes...   \n2    I went to hospital Day 14 after a week of seri...   \n3    I was prescribed a 2 nd course of antibiotics ...   \n4    But ' better ' is still bed ridden , just mean...   \n..                                                 ...   \n464  I also gargled regularly with mouthwash with a...   \n465  I recently had respiratory failure and aspirat...   \n466  I ' m still currently going through data and t...   \n467  Since this virus attacks violently and has a h...   \n468  But it was a rollercoaster and i still have sh...   \n\n                                                 Token    Sentence_ID  \n0    I ' ve had 2 sets of prohylactic antibiotics t...    Sentence #1  \n1    I ' ve had BSYM ISYM during the illness and so...    Sentence #2  \n2    I went to hospital Day 14 after a week of seri...    Sentence #3  \n3    I was prescribed a 2 nd course of antibiotics ...    Sentence #4  \n4    But ' better ' is still BSYM ISYM , just means...    Sentence #5  \n..                                                 ...            ...  \n464  I also gargled regularly with mouthwash with a...  Sentence #465  \n465  I recently had BSYM ISYM and aspiration pneumo...  Sentence #466  \n466  I ' m still currently going through data and t...  Sentence #467  \n467  Since this virus attacks violently and has a h...  Sentence #468  \n468  But it was a rollercoaster and i still have BS...  Sentence #469  \n\n[469 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Token</th>\n      <th>Sentence_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I ' ve had 2 sets of prohylactic antibiotics t...</td>\n      <td>I ' ve had 2 sets of prohylactic antibiotics t...</td>\n      <td>Sentence #1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I ' ve had severe neuropathy during the illnes...</td>\n      <td>I ' ve had BSYM ISYM during the illness and so...</td>\n      <td>Sentence #2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I went to hospital Day 14 after a week of seri...</td>\n      <td>I went to hospital Day 14 after a week of seri...</td>\n      <td>Sentence #3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I was prescribed a 2 nd course of antibiotics ...</td>\n      <td>I was prescribed a 2 nd course of antibiotics ...</td>\n      <td>Sentence #4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But ' better ' is still bed ridden , just mean...</td>\n      <td>But ' better ' is still BSYM ISYM , just means...</td>\n      <td>Sentence #5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>464</th>\n      <td>I also gargled regularly with mouthwash with a...</td>\n      <td>I also gargled regularly with mouthwash with a...</td>\n      <td>Sentence #465</td>\n    </tr>\n    <tr>\n      <th>465</th>\n      <td>I recently had respiratory failure and aspirat...</td>\n      <td>I recently had BSYM ISYM and aspiration pneumo...</td>\n      <td>Sentence #466</td>\n    </tr>\n    <tr>\n      <th>466</th>\n      <td>I ' m still currently going through data and t...</td>\n      <td>I ' m still currently going through data and t...</td>\n      <td>Sentence #467</td>\n    </tr>\n    <tr>\n      <th>467</th>\n      <td>Since this virus attacks violently and has a h...</td>\n      <td>Since this virus attacks violently and has a h...</td>\n      <td>Sentence #468</td>\n    </tr>\n    <tr>\n      <th>468</th>\n      <td>But it was a rollercoaster and i still have sh...</td>\n      <td>But it was a rollercoaster and i still have BS...</td>\n      <td>Sentence #469</td>\n    </tr>\n  </tbody>\n</table>\n<p>469 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colloq_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-23a65bbf",
   "language": "python",
   "display_name": "PyCharm (Covid19)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}